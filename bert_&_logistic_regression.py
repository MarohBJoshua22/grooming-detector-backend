# -*- coding: utf-8 -*-
"""BERT & Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rbEQfMNA__DC12nu5knzd0L-b6JGXIY8
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from transformers import BertTokenizer, BertModel
import torch
from torch.utils.data import DataLoader, Dataset

 
# Load the dataset from a CSV file
df = pd.read_csv('/content/drive/MyDrive/profanity_en.csv')

df = pd.read_csv('/profanity_en.csv')
df_filtered = df[df['category_1'] == 'sexual anatomy / sexual acts']
df_filtered = df_filtered[['text']]
df_filtered['label'] = 1  # Binary label for grooming

df2 = pd.read_csv('/Non-grooming set_utf8.csv')
df2 = df2[['text', 'label']]  # Assume 'label' column exists

# Combine the two datasets
combined_df = pd.concat([df_filtered, df2], ignore_index=True)

# Convert 'label' column to numeric, handling non-numeric values
combined_df['label'] = pd.to_numeric(combined_df['label'], errors='coerce').fillna(0).astype(int)
# 'coerce' replaces non-numeric values with NaN, which are then filled with 0 and converted to integers.

combined_df.shape

combined_df.head(1000)

# Split the data into training and test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    combined_df['text'], combined_df['label'], test_size=0.2, random_state=42
)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Function to encode text using BERT
def encode_texts(texts):
    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**encodings)
    # Use the [CLS] token embedding for classification
    return outputs.last_hidden_state[:, 0, :].numpy()

# Encode the training and test texts
train_embeddings = encode_texts(train_texts)
test_embeddings = encode_texts(test_texts)



# Train a Logistic Regression model on the BERT embeddings
log_reg_model = LogisticRegression(max_iter=1000)
log_reg_model.fit(train_embeddings, train_labels)

# Make predictions on the test set
test_predictions = log_reg_model.predict(test_embeddings)

# Evaluate the model
accuracy = accuracy_score(test_labels, test_predictions)
print(f"Accuracy: {accuracy}")
print(classification_report(test_labels, test_predictions, target_names=['non-grooming', 'grooming']))